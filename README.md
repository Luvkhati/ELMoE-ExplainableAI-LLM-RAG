This project is a framework designed to study how Large Language Models (LLMs) can understand, explain, and validate the behavior of traditional Machine Learning (ML) models. It compares predictions, explanations, and counterfactual examples generated by LLMs with the outputs produced by trained ML models and standard explanation techniques.

The framework works for both regression and classification problems. It uses a clustering-based Retrieval Augmented Generation (RAG) approach, where similar training data points are grouped together and used as contextual information for the LLM. This helps the LLM generate more accurate and meaningful responses instead of relying only on its general knowledge.

For each experiment, the system first runs predictions using the ML model itself. The same test data is then sent to one or more LLMs, which are asked to predict outcomes, explain feature importance, or generate counterfactual inputs. These LLM results are compared with baseline methods such as LIME (for explanations) and DiCE (for counterfactuals) to measure reliability and consistency.

The entire workflow is controlled through a configuration file, making it easy to change models, datasets, metrics, and evaluation settings. The framework also includes automatic evaluation, logging, and visualization of results.

Overall, this project helps researchers and engineers better understand how trustworthy and useful LLMs are when used as explanation tools for machine learning models.
